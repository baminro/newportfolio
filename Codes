###PortfolioAllocationrawprice/volumedatatwo
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')

print("GPU:", tf.config.list_physical_devices('GPU'))
print("Num GPUs:", len(physical_devices))

from google.colab import drive
drive.mount('/content/drive')

# Load libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import read_csv, set_option
from pandas.plotting import scatter_matrix
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import datetime
import math
from numpy.random import choice
import random

from keras.layers import Input, Dense, Flatten, Dropout
from keras.models import Model
from keras.regularizers import l2
import keras
from keras import layers
import numpy as np
import pandas as pd

import random
from collections import deque
import matplotlib.pylab as plt
import matplotlib.dates as mdates # Formatting dates


#Diable the warnings
import warnings
warnings.filterwarnings('ignore')


import os
directory = r'/content/drive/MyDrive/Adjusted'
df=[]
files_in_directory = os.listdir(directory)
filtered_files = [file for file in files_in_directory if file.endswith(".csv")]
for file in filtered_files:
    path_to_file = os.path.join(directory, file)
    aux=pd.read_csv(path_to_file, encoding="utf-16")
    df.append(aux)
df=pd.concat(df)
df['<TICKER>']= df['<TICKER>'].str.replace('-ت', '')
df['<COL12>']= df['<COL12>'].str.replace('-ت', '')
df['<COL12>']= df['<COL12>'].str.replace('_', ' ')
df['<TICKER>']= df['<TICKER>'].str.replace('_', ' ')


df.to_excel('/content/drive/MyDrive/out.xlsx')
import datetime
from datetime import datetime as dt
df['<DTYYYYMMDD>']=df['<DTYYYYMMDD>'].apply(lambda x : dt.strptime(str(x), '%Y%m%d'))


#df['<DTYYYYMMDD>'] = pd.to_datetime(df['<DTYYYYMMDD>']).dt.date
#df['<DTYYYYMMDD>'] = df['<DTYYYYMMDD>'].dt.strftime('%Y%m%d')
df=df.set_index('<DTYYYYMMDD>')
df


df['SMAclose5']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(5).mean())
df['SMAclose30']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(30).mean())
df['EMAclose5']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.ewm(span =5, min_periods = 5 - 1).mean())
df['EMAclose30']=df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.ewm(span =30, min_periods = 5 - 1).mean())
MA = df['SMAclose5']
SD = df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.rolling(window=5).std().mean())
#df['MiddleBandclose'] = MA
df['UpperBandclose'] = MA + (2 * SD)
df['LowerBandclose'] = MA - (2 * SD)

df['FIclose'] =df.groupby('<COL11>')['<CLOSE>'].transform(lambda x: x.diff(1)) * df['<VOL>']

df['dm'] = ((df['<HIGH>'] + df['<LOW>'])/2) - ((df.groupby('<COL11>')['<HIGH>'].transform(lambda x: x.shift(1)) + df.groupby('<COL11>')['<LOW>'].transform(lambda x: x.shift(1)))/2)
df['br'] = (df['<VOL>'] / 100000000) / ((df['<HIGH>'] - df['<LOW>']))
df['EMV'] = df['dm']/ df['br']
df['EMV_MAclose'] =df.groupby('<COL11>')['EMV'].transform(lambda x: x.rolling(5).mean())
del df['dm']
del df['br']


def rsi_transform(price_series, window=5):
    delta = price_series.diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    # Use EMA smoothing (standard for RSI)
    avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()

    rs = avg_gain / avg_loss
    return 100 - (100 / (1 + rs))

# 3. Calculate RSI by ticker using transform
df['RSIclose'] = (df.groupby('<COL11>')['<CLOSE>']
                .transform(rsi_transform, window=5))

df

df=df.reset_index()
df=df[['<DTYYYYMMDD>','<CLOSE>','<OPEN>','<HIGH>','<LOW>','<VOL>','<COL11>','<TICKER>','SMAclose5',	'SMAclose30',	'EMAclose5',	'EMAclose30',	'UpperBandclose',	'LowerBandclose',	'FIclose',	'EMV_MAclose',	'RSIclose']]
df.columns = ['Date','<CLOSE>','<OPEN>','<HIGH>','<LOW>','<VOL>','<COL11>','<TICKER>','SMAclose5',	'SMAclose30',	'EMAclose5',	'EMAclose30',	'UpperBandclose',	'LowerBandclose',	'FIclose',	'EMV_MAclose', 'RSIclose']
df=df[df['Date']<='2025-05-17']
df



df=df.pivot(index='Date', columns='<COL11>', values=['<CLOSE>','<OPEN>','<HIGH>','<LOW>','<VOL>','SMAclose5',	'SMAclose30',	'EMAclose5',	'EMAclose30',	'UpperBandclose',	'LowerBandclose',	'FIclose',	'EMV_MAclose',	'RSIclose'])

df=df.fillna(method='ffill')


col = ('<CLOSE>', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('<OPEN>', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('<HIGH>', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('<LOW>', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('<VOL>', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]


col = ('SMAclose5', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('SMAclose30', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('EMAclose5', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('EMAclose30', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]

col = ('UpperBandclose', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('LowerBandclose', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('FIclose', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
col = ('EMV_MAclose', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]

col = ('RSIclose', 'IDXS')
new_order = [c for c in df.columns if c != col] + [col]
df = df[new_order]
df=df.dropna()
df

dataset=df.dropna()
dataset

dataset=dataset[['<CLOSE>','<OPEN>','<HIGH>','<LOW>','<VOL>']]
col = ('<CLOSE>', 'IDXS')
new_order = [c for c in dataset.columns if c != col] + [col]
dataset = dataset[new_order]
col = ('<OPEN>', 'IDXS')
new_order = [c for c in dataset.columns if c != col] + [col]
dataset = dataset[new_order]
col = ('<HIGH>', 'IDXS')
new_order = [c for c in dataset.columns if c != col] + [col]
dataset = dataset[new_order]
col = ('<LOW>', 'IDXS')
new_order = [c for c in dataset.columns if c != col] + [col]
dataset = dataset[new_order]
col = ('<VOL>', 'IDXS')
new_order = [c for c in dataset.columns if c != col] + [col]
dataset = dataset[new_order]
dataset


dataset.to_csv('/content/drive/MyDrive/crypto_portfolionewpi.csv')
dataindex.to_csv('/content/drive/MyDrive/indexnew.csv')


import numpy as np
import pandas as pd

from IPython.core.debugger import set_trace
from sklearn.preprocessing import MinMaxScaler

#define a function portfolio
def portfolio(returns, weights):
    weights = np.array(weights)
    rets = returns.mean() * 252
    retsmean=returns.mean()
    rets12=returns-retsmean
    lenx=rets12.shape[0]
    rf=0.21/252
    for i in range(lenx):
            if (rets12.iloc[i]>0):
                rets12.loc[:][i]=0
    semicov=((rets12.T.dot(rets12))/(lenx-1))*252
    #covs = returns.cov() * 252
    P_ret = rets
    P_vol =returns.std()* np.sqrt(252)
    P_sharpe = (P_ret-rf) / P_vol
    return np.array([P_ret, P_vol, P_sharpe])


class CryptoEnvironment:

    def __init__(self, prices = '/content/drive/MyDrive/crypto_portfolionewpi.csv' , capital = 1e6):
        self.prices = prices
        self.capital = capital
        self.data = self.load_data()

    def load_data(self):
        data =  pd.read_csv(self.prices)
        try:
            data.index = data['Date']
            data = data.drop(columns = ['Date'])
        except:
            multi_cols = pd.MultiIndex.from_arrays([
                     data.columns,
                     data.iloc[0],
                     data.iloc[1]])
            data.columns = multi_cols

            # Remove the first two rows since they became the new header levels
            data=data.iloc[2:].reset_index(drop=True)
            data.columns = ['_'.join(map(str, col)).strip() for col in data.columns]
            data=data.set_index('Unnamed: 0_<COL11>_Date')
        return data

    def preprocess_state(self, state):
        scaler = MinMaxScaler(feature_range=(0,1))
         # scaling dataset
        state = pd.DataFrame(scaler.fit_transform(state),index=state.index, columns=state.columns)
        return state

    def get_state(self, t, lookback, is_cov_matrix = False, is_raw_time_series = True):

        assert lookback <= t

        decision_making_state = self.data.iloc[t-lookback:t]
        decision_making_state = decision_making_state
        #set_trace()
        if is_cov_matrix:
            x = decision_making_state.cov()
            scaler =MinMaxScaler(feature_range=(0,1))
            # scaling dataset
            x =  pd.DataFrame(scaler.fit_transform(x),index=x.index, columns=x.columns)
            '''retsmean=decision_making_state.mean()
            rets12=decision_making_state-retsmean
            lenx=rets12.shape[0]
            leny=rets12.shape[1]
            for i in range(lenx):
                for j in range(leny):
                    if (rets12.iloc[i,j]>0):
                        rets12.iloc[i,j]=0
            semicov=((rets12.T.dot(rets12))/(lenx-1))
            x=semicov'''
            return x
        elif not is_cov_matrix and not is_raw_time_series:
            x = decision_making_state.cov()
            scaler =MinMaxScaler(feature_range=(0,1))
            # scaling dataset
            #x =  scaler.fit_transform(x)
            decision_making_state = self.data.iloc[t-lookback:t]
            xx = decision_making_state.pct_change().dropna()
            #xx =  scaler.fit_transform(xx)
            xxx =  scaler.fit_transform(np.dot(xx,x))
            xxx= pd.DataFrame(xxx,index=xx.index, columns=xx.columns)

            return xxx
        else:
            if is_raw_time_series:

                decision_making_state = self.data.iloc[t-lookback:t]
                decision_making_state = decision_making_state.apply(pd.to_numeric, errors='coerce')
            return self.preprocess_state(decision_making_state)

    def get_reward(self, action, action_t, reward_t,W,capital, alpha = 0.01):
        def local_portfolio(returns, weights):
            weights = np.array(weights)
            rets = returns.mean() # * 252
            retsmean=returns.mean()
            rets12=returns-retsmean
            lenx=rets12.shape[0]
            rf=0.21/252
            for i in range(lenx):
                      if (rets12.iloc[i]>0):
                            rets12.loc[:][i]=0
            semicov=((rets12.T.dot(rets12))/(lenx-1))
            #covs = returns.cov() # * 252
            P_ret = rets
            P_vol = returns.std()
            P_sharpe = (P_ret-rf) / P_vol
            return np.array([P_ret,P_vol, P_sharpe])

        cost=list(np.array(action) - np.array(W))
        cocd=[element * 0.00371 if element>=0  else abs(element) * 0.0088   for element in cost]
        cocod=sum(cocd)
        data_period = self.data[action_t:reward_t][['<CLOSE>_BANK_nan','<CLOSE>.1_BMLT_nan','<CLOSE>.2_DTIP_nan',
                                                    '<CLOSE>.3_FOLD_nan','<CLOSE>.4_GDIR_nan','<CLOSE>.5_MADN_nan',
                                                    '<CLOSE>.6_MAPN_nan','<CLOSE>.7_MSMI_nan','<CLOSE>.8_NORI_nan',
                                                    '<CLOSE>.9_OIMC_nan','<CLOSE>.10_PASN_nan','<CLOSE>.11_PKLJ_nan',
                                                    '<CLOSE>.12_PNES_nan','<CLOSE>.13_PTAP_nan','<CLOSE>.14_SFKZ_nan',
                                                    '<CLOSE>.15_SSAP_nan','<CLOSE>.16_TAMN_nan','<CLOSE>.17_TSBE_nan']]
        data_period = data_period.apply(pd.to_numeric, errors='coerce')
        cap=capital*np.array(action)
        hj=(data_period/data_period.shift(1))
        #hj=hj.drop(columns=['شاخص كل6'])
        positions=pd.DataFrame(index=hj.index,columns=hj.columns).fillna(0.0)
        r=(positions.head(1))+cap
        pd.concat([r,hj])
        hjj=hj.drop(hj.index[:1])
        kl=pd.concat([r,hjj])
        kll=kl.cumprod(skipna=True)
        kll=kll.fillna(method='ffill')
        kll['hold']=kll.sum(axis=1)
        rf=0.21/252
        rr=(r.sum(axis=1).values)
        kll['cash']=0
        kll['cash']=(capital-kll['hold'])
        kll['cash']=kll['cash'].copy()
        for d in range(1,len(kll.index)):
            kll['cash'].iloc[d]=(1+rf)

        kll['cashcum']=kll['cash'].cumprod()
        kll['port']=kll['cashcum']+kll['hold']


        capital =kll['port'].iloc[-1]
        W=list((kll[data_period.columns].iloc[-1]/kll['port'].iloc[-1]).values)
        kll['port'][0]=kll['port'][0]-cocod*kll['port'][0]

        #data_period = self.data[action_t:reward_t]
        weights = action
        #returns = data_period.pct_change().dropna()
        returns =  kll['port'].pct_change().dropna()
        sharpe = local_portfolio(returns, weights)[-1]
        sharpe = np.array([sharpe] * (len(self.data.columns)-77))
        #rew = (data_period.values[-1] - data_period.values[0]) / data_period.values[0]

        return returns, sharpe, W,capital



import numpy as np
import tensorflow as tf
import keras.backend as K

class Attention(tf.keras.layers.Layer):
          def __init__(self, units):
              super(Attention, self).__init__()
              self.W = tf.keras.layers.Dense(units)
              self.V = tf.keras.layers.Dense(1)
          def call(self, inputs):
              # Compute attention scores
              score = tf.nn.tanh(self.W(inputs))
              attention_weights = tf.nn.softmax(self.V(score), axis=1)

              # Apply attention weights to input
              context_vector = attention_weights * inputs
              context_vector = tf.reduce_sum(context_vector, axis=1)

              return context_vector

class Agent:

    def __init__(
                     self,
                     portfolio_size,
                     is_eval = False,
                     allow_short = False,
                 ):

        self.portfolio_size = portfolio_size
        self.allow_short = allow_short
        self.input_shape = (50,95, )
        self.action_size = 3 # sit, buy, sell

        self.memory4replay = []
        self.is_eval = is_eval

        self.alpha = 0.85
        self.gamma = 0.50
        self.epsilon = 1
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.99

        self.model = self._model()


    def _model(self):

        inputs = Input(shape=self.input_shape)
        #Flatten()
        #x = Flatten()(inputs)
        x=keras.layers.Conv1D(filters=256, kernel_size=2,
                      strides=1, padding="valid",
                      activation="relu")(inputs)
        #x =Flatten()(x)

        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding="valid")(x)
        x=keras.layers.Conv1D(filters=256, kernel_size=2,
                      strides=1, padding="valid",
                      activation="relu")( inputs)
        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding="valid")(x)
        x=keras.layers.Conv1D(filters=256, kernel_size=2,
                      strides=1, padding="valid",
                      activation="relu")( inputs)
        x=keras.layers.MaxPooling1D(pool_size=2,strides=1, padding="valid")(x)


        #x=keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True),
                              # backward_layer=tf.keras.layers.LSTM(64, activation='relu', return_sequences=True, go_backwards=True),
                              #merge_mode="concat")(x)
        #x=keras.layers.Conv1D(filters=32, kernel_size=2,
                      #strides=1, padding="valid",
                      #activation="relu")(x)
       # x=keras.layers.MaxPooling1D(pool_size=2,strides=2, padding="valid")(x)
        #Attention(64),
        #tf.keras.layers.Reshape((128,1)),
        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(126, return_sequences=True),
                                 #backward_layer=tf.keras.layers.LSTM(126, activation='relu', return_sequences=True, go_backwards=True),
                                 #merge_mode="concat"),
        x = Dense(256, activation='elu')(x)

        #
        #x=keras.layers.LSTM(256,activation='relu',return_sequences=False)(x)
        #x=keras.layers.Reshape((256,1))(x)
        #x=keras.layers.LSTM(30,activation='relu',return_sequences=False)(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='elu')(x)
        x = Dropout(0.5)(x)
        x = Flatten()(x)

        predictions = []
        for i in range(self.portfolio_size-77):
            asset_dense = Dense(self.action_size, activation='linear')(x)
            predictions.append(asset_dense)

        model = Model(inputs=inputs, outputs=predictions)
        model.compile(optimizer='adam', loss='mse')
        return model

    def nn_pred_to_weights(self, pred, allow_short = False):

        weights = np.zeros(len(pred))
        raw_weights = np.argmax(pred, axis=-1)

        saved_min = None

        for e, r in enumerate(raw_weights):
            if r == 0: # sit
                weights[e] = 0
            elif r == 1: # buy
                weights[e] = np.abs(pred[e][0][r])
            else:
                weights[e] = -np.abs(pred[e][0][r])
        #sum of absolute values in short is allowed
        if not allow_short:
            weights += np.abs(np.min(weights))
            saved_min = np.abs(np.min(weights))
            saved_sum = np.sum(weights)
        else:
            saved_sum = np.sum(np.abs(weights))

        weights /= saved_sum
        return weights, saved_min, saved_sum
    #return the action based on the state, uses the NN function
    def act(self, state):

        if not self.is_eval and random.random() <= self.epsilon:
            w = np.random.normal(0, 1, size = (self.portfolio_size-77, ))

            saved_min = None

            if not self.allow_short:
                w += np.abs(np.min(w))
                saved_min = np.abs(np.min(w))

            saved_sum = np.sum(w)
            w /= saved_sum
            return w , saved_min, saved_sum
        pred = self.model.predict(np.expand_dims(state.values, 0))
        return self.nn_pred_to_weights(pred, self.allow_short)

    def expReplay(self, batch_size):

        def weights_to_nn_preds_with_reward(action_weights,
                                            reward,
                                            Q_star = np.zeros((self.portfolio_size-77, self.action_size))):

            Q = np.zeros((self.portfolio_size-77, self.action_size))
            for i in range(self.portfolio_size-77):
                if action_weights[i] == 0:
                    Q[i][0] = reward[i] + self.gamma * np.max(Q_star[i][0])
                elif action_weights[i] > 0:
                    Q[i][1] = reward[i] + self.gamma * np.max(Q_star[i][1])
                else:
                    Q[i][2] = reward[i] + self.gamma * np.max(Q_star[i][2])
            return Q

        def restore_Q_from_weights_and_stats(action):
            action_weights, action_min, action_sum = action[0], action[1], action[2]
            action_weights = action_weights * action_sum
            if action_min != None:
                action_weights = action_weights - action_min
            return action_weights

        for (s, s_, action, reward, done) in self.memory4replay:

            action_weights = restore_Q_from_weights_and_stats(action)
            #Reward =reward if not in the terminal state.
            Q_learned_value = weights_to_nn_preds_with_reward(action_weights, reward)
            s, s_ = s.values, s_.values

            if not done:
                # reward + gamma * Q^*(s_, a_)
                Q_star = self.model.predict(np.expand_dims(s_, 0))
                Q_learned_value = weights_to_nn_preds_with_reward(action_weights, reward, np.squeeze(Q_star))

            Q_learned_value = [xi.reshape(1, -1) for xi in Q_learned_value]
            Q_current_value = self.model.predict(np.expand_dims(s, 0))
            Q = [np.add(a * (1-self.alpha), q * self.alpha) for a, q in zip(Q_current_value, Q_learned_value)]

            # update current Q function with new optimal value

            self.model.fit(np.expand_dims(s, 0), Q, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            Q_learned_value = [xi.reshape(1, -1) for xi in Q_learned_value]
            Q_current_value = self.model.predict(np.expand_dims(s, 0))
            Q = [np.add(a * (1-self.alpha), q * self.alpha) for a, q in zip(Q_current_value, Q_learned_value)]

            # update current Q function with new optimal value

            self.model.fit(np.expand_dims(s, 0), Q, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


N_ASSETS = 95 #53
agent = Agent(N_ASSETS)
env = CryptoEnvironment()


window_size = 50
episode_count =10
batch_size = 32
rebalance_period =5 #every 90 days weight change

### Training the Data
for e in range(episode_count):

    agent.is_eval = False
    data_length = len(env.data)

    returns_history = []
    returns_history_equal = []

    rewards_history = []
    equal_rewards = []

    actions_to_show = []
    actions_to_show1 = []
    diffc=[]
    print("Episode " + str(e) + "/" + str(episode_count), 'epsilon', agent.epsilon)

    s = env.get_state(np.random.randint(window_size+1, data_length-505-window_size-1), window_size)
    total_profit = 0
    W0=(N_ASSETS-77)*[ 1./(N_ASSETS-77)]
    W0 = [element * 0 for element in W0]
    W0eq=(N_ASSETS-77)*[ 1./(N_ASSETS-77)]
    W0eq = [element * 0 for element in W0eq]
    initial_capital=float(1000)
    initial_capitaleq=float(1000)
    for t in range(window_size, data_length-505, rebalance_period):
        date1 = t-rebalance_period
        #correlation from 90-180 days
        s_ = env.get_state(t, window_size)
        action = agent.act(s_)

        if t==window_size:
           actions_to_show1=np.zeros(N_ASSETS-77)
        else:
           actions_to_show1=actions_to_show[-1]
        actions_to_show.append(action[0])
        diffc.append(action[0]-actions_to_show1)

        weighted_returns, reward,WW0,initial_capital1 = env.get_reward(action[0], date1, t,W0,initial_capital)
        weighted_returns_equal, reward_equal,WW0eq,initial_capital1eq = env.get_reward(
            np.ones((agent.portfolio_size)-77) / ((agent.portfolio_size)-77), date1, t,W0eq,initial_capitaleq)

        rewards_history.append(reward)
        equal_rewards.append(reward_equal)
        returns_history.extend(weighted_returns)
        returns_history_equal.extend(weighted_returns_equal)
        W0=WW0
        initial_capital=initial_capital1
        W0eq=WW0eq
        initial_capitaleq=initial_capital1eq
        done = True if t == data_length else False
        agent.memory4replay.append((s, s_, action, reward, done))

        if len(agent.memory4replay) >= batch_size:
            agent.expReplay(batch_size)
            agent.memory4replay = []

        s = s_


    rl_result = np.array(returns_history).cumsum()
    equal_result = np.array(returns_history_equal).cumsum()

    plt.figure(figsize = (12, 2))
    plt.plot(rl_result, color = 'black', ls = '-')
    plt.plot(equal_result, color = 'grey', ls = '--')
    plt.show()

    plt.figure(figsize = (12, 2))
    for a in actions_to_show:
        plt.bar(np.arange(N_ASSETS-77), a, color = 'grey', alpha = 0.25)
        plt.xticks(np.arange(N_ASSETS-77), env.data[['<CLOSE>_BANK_nan','<CLOSE>.1_BMLT_nan','<CLOSE>.2_DTIP_nan',
                                                    '<CLOSE>.3_FOLD_nan','<CLOSE>.4_GDIR_nan','<CLOSE>.5_MADN_nan',
                                                    '<CLOSE>.6_MAPN_nan','<CLOSE>.7_MSMI_nan','<CLOSE>.8_NORI_nan',
                                                    '<CLOSE>.9_OIMC_nan','<CLOSE>.10_PASN_nan','<CLOSE>.11_PKLJ_nan',
                                                    '<CLOSE>.12_PNES_nan','<CLOSE>.13_PTAP_nan','<CLOSE>.14_SFKZ_nan',
                                                    '<CLOSE>.15_SSAP_nan','<CLOSE>.16_TAMN_nan','<CLOSE>.17_TSBE_nan']].columns, rotation='vertical')
    plt.show()



###Testing the Data
agent.is_eval = True
meanresult_equal, meanresult_rl=[],[]
actions_equal, actions_rl = [], []
result_equal, result_rl = [], []
cccc=[]
actions_to_showtest=[]

W0=(N_ASSETS-77)*[ 1./(N_ASSETS-77)]
W0 = [element * 0 for element in W0]
W0eq=(N_ASSETS-77)*[ 1./(N_ASSETS-77)]
W0eq = [element * 0 for element in W0eq]
initial_capital=float(1000)
initial_capitaleq=float(1000)
for t in range(len(env.data)-505, len(env.data), rebalance_period):

    date1 = t-rebalance_period
    s_ = env.get_state(t, window_size)
    action = agent.act(s_)
    actions_to_showtest.append(action[0])

    weighted_returns, reward,WW0,initial_capital1 = env.get_reward(action[0], date1, t,W0,initial_capital)
    weighted_returns_equal, reward_equal,WW0eq,initial_capital1eq = env.get_reward(
        np.ones((agent.portfolio_size)-77) / ((agent.portfolio_size)-77), date1, t,W0eq,initial_capitaleq)
    W0=WW0
    initial_capital=initial_capital1
    W0eq=WW0eq
    initial_capitaleq=initial_capital1eq
    result_equal.append(weighted_returns_equal.tolist())
    meanresult_equal.append(np.mean(np.array(weighted_returns_equal)))
    actions_equal.append(np.ones((agent.portfolio_size)-77) / ((agent.portfolio_size)-77))
    cccc.append(date1)
    result_rl.append(weighted_returns.tolist())
    meanresult_rl.append(np.mean(np.array(weighted_returns)))
    actions_rl.append(action[0])


dataind = read_csv('/content/drive/MyDrive/indexnew.csv',index_col=0,header=2)
#dataind[len(env.data)-756:len(env.data)]
dataind= dataind.rename(columns={'Unnamed: 1': 'IDXS'})
dataind=dataind[len(env.data)-750:len(env.data)]
dataind=dataind.reset_index()

fgh=[]
fgh1=[]
for c in range(rebalance_period,len(dataind.index)+1,rebalance_period):
         fgh.append(dataind['IDXS'][c-rebalance_period:c])

         #fgh.append(dataind1['IDXS'][rebalance_period:len(dataind1.index)])

x111=[]
for n in range(0,len(fgh)):
    x1=pd.DataFrame(((fgh[n]-fgh[n].shift(1))/fgh[n].shift(1)).dropna())
    x111.append(x1)
'''x1=pd.DataFrame(((fgh[1]-fgh[1].shift(1))/fgh[1].shift(1)).dropna())
x2=pd.DataFrame(((fgh[2]-fgh[2].shift(1))/fgh[2].shift(1)).dropna())
x3=pd.DataFrame(((fgh[3]-fgh[3].shift(1))/fgh[3].shift(1)).dropna())'''

#xxx=np.concatenate([x111])
xxx=np.concatenate(x111)


result_equal_vis = [item for sublist in result_equal for item in sublist]
result_rl_vis = [item for sublist in result_rl for item in sublist]


# Importing library
import scipy.stats as stats

# pre holds the mileage before
# applying the different engine oil
pre = result_equal_vis

# post holds the mileage after
# applying the different engine oil
post = result_rl_vis
# Performing the paired sample t-test
stats.ttest_rel(pre, post,alternative='less')


plt.figure(figsize = (10, 5))
plt.plot(np.array(result_equal_vis).cumsum(), label = 'Benchmark', color = 'grey',ls = '--')
plt.plot(np.array(result_rl_vis).cumsum(), label = 'Deep RL portfolio', color = 'black',ls = '-')
plt.plot(np.array(xxx).cumsum(), label = 'Index', color = 'green',ls = '-.')
df11133445566=pd.DataFrame(np.array(xxx).cumsum(),columns=['Index']).join(pd.DataFrame(np.array(result_equal_vis).cumsum(),columns=['Benchmark']))
df1113344556677=pd.DataFrame(np.array(result_rl_vis).cumsum(),columns=['Deep RL portfolio']).join(df11133445566)
for var in list(['Benchmark','Deep RL portfolio','Index']):
    plt.annotate('%0.2f' % df1113344556677[var].iloc[-1], xy=(1, df1113344556677[var].iloc[-1]), xytext=(8, 0),
                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va="center")
plt.xlabel('Date')
# Set the y axis label of the current axis.
plt.ylabel('Cumluative_Return')
# Set a title of the current axes.
plt.title('Comparison of Performance')
# show a legend on the plot
plt.legend()
plt.show()

#Plotting the data
import matplotlib
current_cmap = matplotlib.cm.get_cmap()
current_cmap.set_bad(color='red')

N = len(np.array([item for sublist in result_equal for item in sublist]).cumsum())

for i in range(0, len(actions_rl)):
    current_range = np.arange(0, N)
    current_ts = np.zeros(N)
    current_ts2 = np.zeros(N)

    ts_benchmark = np.array([item for sublist in result_equal[:i+1] for item in sublist]).cumsum()
    ts_target = np.array([item for sublist in result_rl[:i+1] for item in sublist]).cumsum()

    t = len(ts_benchmark)
    current_ts[:t] = ts_benchmark
    current_ts2[:t] = ts_target

    current_ts[current_ts == 0] = ts_benchmark[-1]
    current_ts2[current_ts2 == 0] = ts_target[-1]

    plt.figure(figsize = (12, 10))

    plt.subplot(2, 1, 1)
    plt.bar(np.arange(N_ASSETS-77), actions_rl[i], color = 'grey')
    plt.xticks(np.arange(N_ASSETS-77), env.data[['<CLOSE>_BANK_nan','<CLOSE>.1_BMLT_nan','<CLOSE>.2_DTIP_nan',
                                                    '<CLOSE>.3_FOLD_nan','<CLOSE>.4_GDIR_nan','<CLOSE>.5_MADN_nan',
                                                    '<CLOSE>.6_MAPN_nan','<CLOSE>.7_MSMI_nan','<CLOSE>.8_NORI_nan',
                                                    '<CLOSE>.9_OIMC_nan','<CLOSE>.10_PASN_nan','<CLOSE>.11_PKLJ_nan',
                                                    '<CLOSE>.12_PNES_nan','<CLOSE>.13_PTAP_nan','<CLOSE>.14_SFKZ_nan',
                                                    '<CLOSE>.15_SSAP_nan','<CLOSE>.16_TAMN_nan','<CLOSE>.17_TSBE_nan']].columns, rotation='vertical')
    plt.xlabel('Stock')
    # Set the y axis label of the current axis.
    plt.ylabel('RL_Action')
    # Set a title of the current axes.
    plt.title('Comparison of Performance')

    plt.subplot(2, 1, 2)
    plt.colormaps = current_cmap
    plt.plot((current_range[:t]), current_ts[:t], color = 'black', label = 'Benchmark')
    plt.plot((current_range[:t]), current_ts2[:t], color = 'red', label = 'Deep RL portfolio')
    plt.plot((current_range[t:]), current_ts[t:], ls = '--', lw = .1, color = 'black')
    #plt.autoscale(False)
    plt.ylim([-0.8, 0.8])
    plt.xlabel('Date')
    # Set the y axis label of the current axis.
    plt.ylabel('Cumluative_Return')
    # Set a title of the current axes.
    #plt.title('Comparison of Performance')
    # show a legend on the plot
    plt.legend()


import statsmodels.api as sm
from statsmodels import regression
rf=0.21
def sharpe(R):
    #r = np.diff(R)
    sr = (R.mean()*252-rf)/(R.std() * np.sqrt(252))
    return sr

def sortino_ratio(series):
    #series = np.diff(series)
    mean = series.mean() * 252
    std_neg = series[series<0].std() * np.sqrt(252)
    sor=(mean-rf)/std_neg
    return sor

def max_drawdown(return_series):
    comp_ret = pd.Series((return_series+1).cumprod())
    peak = comp_ret.expanding(min_periods=1).max()
    dd = (comp_ret/peak)-1
    return dd.min()

def print_stats(result, benchmark):

    sharpe_ratio = sharpe(np.array(result))
    sor = sortino_ratio(np.array(result))
    max_draw=max_drawdown(np.array(result))
    returns = np.mean(np.array(result)*252)
    volatility = np.std(np.array(result)*np.sqrt(252))
    calmars = (returns-rf)/abs(max_draw)

    X = benchmark
    y = result
    x = sm.add_constant(X)
    model = regression.linear_model.OLS(y, x).fit()
    alpha = model.params[0]*252
    beta = model.params[1]



    return np.round(np.array([returns, volatility, sharpe_ratio,sor,max_draw,calmars, alpha, beta]), 4).tolist()


print('EQUAL', print_stats(result_equal_vis, xxx))
print('RL AGENT', print_stats(result_rl_vis, xxx))
print('index', print_stats(xxx, xxx))

pd.DataFrame(result_rl_vis).to_csv('/content/drive/MyDrive/result_rl_visptwo.csv')

rl_vispione= read_csv('/content/drive/MyDrive/result_rl_vispione.csv',index_col=0)
rl_vispione.columns = ['result_rl_vispione']
rl_vispone= read_csv('/content/drive/MyDrive/result_rl_vispone.csv',index_col=0)
rl_vispone.columns = ['result_rl_vispone']


rl_vispitwo= read_csv('/content/drive/MyDrive/result_rl_vispitwo.csv',index_col=0)
rl_vispitwo.columns = ['result_rl_vispitwo']
rl_visptwo= read_csv('/content/drive/MyDrive/result_rl_visptwo.csv',index_col=0)
rl_visptwo.columns = ['result_rl_visptwo']

rl_vispithree= read_csv('/content/drive/MyDrive/result_rl_vispithree.csv',index_col=0)
rl_vispithree.columns = ['result_rl_vispithree']
rl_vispthree= read_csv('/content/drive/MyDrive/result_rl_vispthree.csv',index_col=0)
rl_vispthree.columns = ['result_rl_vispthree']

vnp=pd.concat([rl_vispitwo.cumsum(),rl_visptwo.cumsum()],axis=1)
plt.figure(figsize = (10, 5))
plt.plot(rl_vispitwo.cumsum(), label = 'rl_vispitwo',ls = '--')
plt.plot(rl_visptwo.cumsum(), label = 'rl_visptwo',ls = '-')

for var in list(vnp.columns):
    plt.annotate('%0.2f' % vnp[var].iloc[-1], xy=(1, vnp[var].iloc[-1]), xytext=(8, 0),
                 xycoords=('axes fraction', 'data'), textcoords='offset points',size=10, va="center")
plt.xlabel('Date')
# Set the y axis label of the current axis.
plt.ylabel('Cumluative_Return')
# Set a title of the current axes.
plt.title('Comparison of Performance')
# show a legend on the plot
plt.legend()
plt.show()

# Importing library
import scipy.stats as stats

# pre holds the mileage before
# applying the different engine oil
pre = rl_visptwo

# post holds the mileage after
# applying the different engine oil
post = rl_vispitwo
# Performing the paired sample t-test
stats.ttest_rel(pre, post,alternative='less')
